{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comments Classification\n",
    "### ML First Iteration_ Assignment 1\n",
    "\n",
    "#### The following notebook documents the first iteration on the toxic comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports and magic commands\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from my_measures import BinaryClassificationPerformance\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#added countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BinaryClassificationPerformance in module my_measures:\n",
      "\n",
      "class BinaryClassificationPerformance(builtins.object)\n",
      " |  BinaryClassificationPerformance(predictions, labels, desc, probabilities=None)\n",
      " |  \n",
      " |  Performance measures to evaluate the fit of a binary classification model, v1.02\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, predictions, labels, desc, probabilities=None)\n",
      " |      Initialize attributes: predictions-vector of predicted values for Y, labels-vector of labels for Y\n",
      " |  \n",
      " |  compute_measures(self)\n",
      " |      Compute performance measures defined by Flach p. 57\n",
      " |  \n",
      " |  img_indices(self)\n",
      " |      Get the indices of true and false positives to be able to locate the corresponding images in a list of image names\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BinaryClassificationPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for feature building and extraction on Natural Language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes raw data and completes all preprocessing required before model fits\n",
    "def process_raw_data(fn, my_random_seed, test=False):\n",
    "    # read and summarize data\n",
    "    toxic_data = pd.read_csv(fn)\n",
    "    if (not test):\n",
    "        # add an indicator for any toxic, severe toxic, obscene, threat, insult, or indentity hate\n",
    "        toxic_data['any_toxic'] = (toxic_data['toxic'] + toxic_data['severe_toxic'] + toxic_data['obscene'] + toxic_data['threat'] + toxic_data['insult'] + toxic_data['identity_hate'] > 0)\n",
    "    print(\"toxic_data is:\", type(toxic_data))\n",
    "    print(\"toxic_data has\", toxic_data.shape[0], \"rows and\", toxic_data.shape[1], \"columns\", \"\\n\")\n",
    "    print(\"the data types for each of the columns in toxic_data:\")\n",
    "    print(toxic_data.dtypes, \"\\n\")\n",
    "    print(\"the first 10 rows in toxic_data:\")\n",
    "    print(toxic_data.head(5))\n",
    "    if (not test):\n",
    "        print(\"The rate of 'toxic' Wikipedia comments in the dataset: \")\n",
    "        print(toxic_data['any_toxic'].mean())\n",
    "\n",
    "    # vectorize Bag of Words from review text; as sparse matrix\n",
    "    if (not test): # fit_transform()\n",
    "        hv = HashingVectorizer(n_features=2 ** 13, alternate_sign=False)\n",
    "        X_hv = hv.fit_transform(toxic_data.comment_text)\n",
    "        fitted_transformations.append(hv) #what is this doing?\n",
    "        print(\"Shape of HashingVectorizer X:\")    \n",
    "        print(X_hv.shape)\n",
    "    else: # transform() \n",
    "        X_hv = fitted_transformations[0].transform(toxic_data.comment_text)\n",
    "        print(\"Shape of HashingVectorizer X:\")\n",
    "        print(X_hv.shape)\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "    if (not test):\n",
    "        transformer = TfidfTransformer()\n",
    "        X_tfidf = transformer.fit_transform(X_hv)\n",
    "        fitted_transformations.append(transformer)\n",
    "    else:\n",
    "        X_tfidf = fitted_transformations[1].transform(X_hv)\n",
    "    \n",
    "    # create additional quantitative features\n",
    "    # features from Amazon.csv to add to feature set\n",
    "    toxic_data['word_count'] = toxic_data['comment_text'].str.split(' ').str.len()\n",
    "    toxic_data['punc_count'] = toxic_data['comment_text'].str.count(\"\\!\")\n",
    "    # should it be a boolean value? As str.find returns an index; how will a boolean be converted to 0 or 1\n",
    "    toxic_data['occur_fuck'] =(toxic_data['comment_text'].str.lower().str.count(\"fuck\"))\n",
    "#     toxic_data['occur_cock'] =(toxic_data['comment_text'].str.lower().str.count(\"cock\"))\n",
    "    toxic_data['occur_shit'] =(toxic_data['comment_text'].str.lower().str.count(\"shit\"))\n",
    "    toxic_data['uppercase_letters'] = toxic_data['comment_text'].str.count(r'[A-Z]')\n",
    "    \n",
    "\n",
    "    X_quant_features = toxic_data[[ \"word_count\",\"occur_fuck\", \"occur_shit\", \"punc_count\", \"uppercase_letters\"]]\n",
    "    print(\"Look at a few rows of the new quantitative features: \")\n",
    "    print(X_quant_features.head(10))\n",
    "    \n",
    "    # Combine all quantitative features into a single sparse matrix\n",
    "    X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "    X_combined = hstack([X_tfidf, X_quant_features_csr])\n",
    "    X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "    print(\"Size of combined bag of words and new quantitative variables matrix:\")\n",
    "    print(X_matrix.shape)\n",
    "    \n",
    "    # Create `X`, scaled matrix of features\n",
    "    # feature scaling\n",
    "    if (not test):\n",
    "        sc = StandardScaler(with_mean=False)\n",
    "        X = sc.fit_transform(X_matrix)\n",
    "        fitted_transformations.append(sc)\n",
    "        print(X.shape)\n",
    "        y = toxic_data['any_toxic']\n",
    "    else:\n",
    "        X = fitted_transformations[2].transform(X_matrix)\n",
    "        print(X.shape)\n",
    "    \n",
    "    # Create Training and Test Sets\n",
    "    # enter an integer for the random_state parameter; any integer will work\n",
    "    if (test):\n",
    "        X_submission_test = X\n",
    "        print(\"Shape of X_test for submission:\")\n",
    "        print(X_submission_test.shape)\n",
    "        print('SUCCESS!')\n",
    "        return(toxic_data, X_submission_test)\n",
    "    else: \n",
    "        X_train, X_test, y_train, y_test, X_raw_train, X_raw_test = train_test_split(X, y, toxic_data, test_size=0.2, random_state=my_random_seed)\n",
    "        print(\"Shape of X_train and X_test:\")\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(\"Shape of y_train and y_test:\")\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        print(\"Shape of X_raw_train and X_raw_test:\")\n",
    "        print(X_raw_train.shape)\n",
    "        print(X_raw_test.shape)\n",
    "        print('SUCCESS!')\n",
    "        return(X_train, X_test, y_train, y_test, X_raw_train, X_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_data is: <class 'pandas.core.frame.DataFrame'>\n",
      "toxic_data has 159571 rows and 9 columns \n",
      "\n",
      "the data types for each of the columns in toxic_data:\n",
      "id               object\n",
      "comment_text     object\n",
      "toxic             int64\n",
      "severe_toxic      int64\n",
      "obscene           int64\n",
      "threat            int64\n",
      "insult            int64\n",
      "identity_hate     int64\n",
      "any_toxic          bool\n",
      "dtype: object \n",
      "\n",
      "the first 10 rows in toxic_data:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  any_toxic  \n",
      "0             0        0       0       0              0      False  \n",
      "1             0        0       0       0              0      False  \n",
      "2             0        0       0       0              0      False  \n",
      "3             0        0       0       0              0      False  \n",
      "4             0        0       0       0              0      False  \n",
      "The rate of 'toxic' Wikipedia comments in the dataset: \n",
      "0.10167887648758234\n",
      "Shape of HashingVectorizer X:\n",
      "(159571, 8192)\n",
      "Look at a few rows of the new quantitative features: \n",
      "   word_count  occur_fuck  occur_shit  punc_count  uppercase_letters\n",
      "0          42           0           0           0                 17\n",
      "1          18           0           0           1                  8\n",
      "2          42           0           0           0                  4\n",
      "3         112           0           0           0                 11\n",
      "4          13           0           0           0                  2\n",
      "5          12           0           0           0                  1\n",
      "6           8           0           0           0                 37\n",
      "7          21           0           0           0                  4\n",
      "8          83           0           0           0                  7\n",
      "9          12           0           0           0                  2\n",
      "Size of combined bag of words and new quantitative variables matrix:\n",
      "(159571, 8197)\n",
      "(159571, 8197)\n",
      "Shape of X_train and X_test:\n",
      "(127656, 8197)\n",
      "(31915, 8197)\n",
      "Shape of y_train and y_test:\n",
      "(127656,)\n",
      "(31915,)\n",
      "Shape of X_raw_train and X_raw_test:\n",
      "(127656, 14)\n",
      "(31915, 14)\n",
      "SUCCESS!\n",
      "Number of fits stored in `fitted_transformations` list: \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store any use of fit_transform() to transform() later\n",
    "# it is a global list to store model and feature extraction fits\n",
    "fitted_transformations = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_raw_train, X_raw_test = process_raw_data(fn='toxiccomments_train.csv', my_random_seed=56)\n",
    "\n",
    "print(\"Number of fits stored in `fitted_transformations` list: \")\n",
    "print(len(fitted_transformations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model : SVM, Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 12926, 'Neg': 114730, 'TP': 9402, 'TN': 113660, 'FP': 1070, 'FN': 3524, 'Accuracy': 0.9640126590211192, 'Precision': 0.8978227654698243, 'Recall': 0.7273711898499149, 'desc': 'svm_train'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "svm = linear_model.SGDClassifier(max_iter=2000, alpha=0.01)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_performance_train = BinaryClassificationPerformance(svm.predict(X_train), y_train, 'svm_train')\n",
    "svm_performance_train.compute_measures()\n",
    "print(svm_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of false positives:\n",
      "213\n",
      "-Thank God, or we might still hear more of his crap.\n",
      "* * * * * * * * * \n",
      "2509\n",
      "why are you defending a hate group? why are you on their side?\n",
      "* * * * * * * * * \n",
      "2555\n",
      "userpage \n",
      "\n",
      "thanks for shifting everything around. how do i get a page like yours\n",
      "* * * * * * * * * \n",
      "8180\n",
      "Are you Rex071404 or not?\n",
      "* * * * * * * * * \n",
      "9409\n",
      "Thanks no idea how that was created. I was trying to insert a picture into my user page with the Android Wiki beta app. Apologies. Either I'm blatantly stupid or the app is buggy!\n",
      "* * * * * * * * * \n",
      "9479\n",
      "Umm, you confirm the representativeness of the sample by repeating the experiment. This is high school science right? It's almost painful that shit like you gets to write this encyclopedia. 86.176.7.174\n",
      "* * * * * * * * * \n",
      "9824\n",
      "Gz on male nurse job XD\n",
      "* * * * * * * * * \n",
      "10457\n",
      "nonsense \n",
      "\n",
      "are you a boy or a girl\n",
      "* * * * * * * * * \n",
      "12032\n",
      "|listas = Rapson, Ralph\n",
      "* * * * * * * * * \n",
      "14726\n",
      "\" He and a bunch of his fellow \"\"volunteers\"\" are a real self-entitled, arrogant, condescending bunch. It's very reassuring that wikipedia is run by people like this. \"\n",
      "* * * * * * * * * \n",
      "18376\n",
      "\"Some liberal kook writes \"\"A standalone \"\"Credibility\"\" section would be too hurtful to her and her family.\"\"\n",
      "\n",
      "Hurtful to her family?? This woman is LYING through her teeth about being an Indian. And she is running to be one of only 100 US Senators. Is whether or not something is hurtful 'to her and her family' (meaning of course to her CAMPAIGN!!) the standard you use when you collect all the TRASH you possibly can on Michelle Malkin, Ann Coulter, Bill O'Reilly, Rush Limbaugh, Dick Cheney etc etc etc???\n",
      "Bahahahah 99.185.56.156  \n",
      "\n",
      "\"\n",
      "* * * * * * * * * \n",
      "18545\n",
      "Yeah I know, it was my friends this time that were vandalizing after I told them what was going on; they didn't listen to me apparently.\n",
      "* * * * * * * * * \n",
      "22071\n",
      "\"\n",
      "\n",
      " A philosophical story to embrace ones mind. \n",
      "\n",
      "The ancient Indian mathematicians were the first to discover the digit zero \"\"0\"\" and implement it in the formation of digits. The whole world revolves like a cycle in the form of the digit zero. The Britishers dicovered an ancient egg of Dinosaures in India. What I mean to convey is that: India is the land where Lord Rama or Lord Krishna were born. Lord Rama was an example of a perfect ideal man on earth. His life story is an example for an ideal man to follow. Lord Rama, Lord Lakshmana, Lord Bharata and Lord Shatrughana were four sons of King Dhasratha of Ayodhya. He was cursed to die from the pain of separation from his son by the blind parents of Shrawan Kumar. Kaikeyi the mother of Bharata wishes to send Lord Rama on an exile for a period of 14 years and wishes her son Bharata to be the King of Ayodhya. Lord Rama and his wife Sita and his brother Lord Lakshmana leave for forests outside the kingdom of Ayodhya. Lord Rama's brother Lord Bharata persuades lord Rama to return back but lord Rama refuses to return to Ayodhya against the wishes of his step mother Kaikeyi. But Lord Bharata returns to Ayodhya along with the sandals of lord Rama and places the sandals of his brother Lord Rama on the throne and rules the kingdom of Ayodhaya in the name of Rama. This shows the example of love between an elder brother and his younger brother. In the forest once a demon shurpanaka sister of King Ravana disuades Lord Rama to marry her, but Lord Rama tells her that he is already married and points towards his brother Lord Lakshmana who cuts off her nose and ears to show her ugly nature and character. Shurphanaka runs off and complains about Lord Rama and Lord Lakshamana to King Ravana of Lanka. King Ravana somehow manages to kidnap Goddess Sita the wife of Lord Rama in revenge to Lanka. Lord Rama grieves for his separation from his beloved wife Goddess Sita. Here the feelings of an ideal person in the form of lord grieving from separation from his beloved wife are beautifully described by Sage Valmiki who wrote the whole Ramayana. Sage Valmiki was a thief named Ratnakar who by the japam of Lord Sri Rama became a great sage named Valmiki and wrote a great epic called Ramanaya the story of King Rama and Sita. Lord Rama and Lord Lakshamana move towards southern India where they meet the lord Hanuman, (the great son of Anjani and Kesari; Lord Hanumanji is also known as Pavanputra or the great celibate bachelor) and Lord Sugriva and with the help of vanara sena or Army of Monkeys they move into the kingdom of Lanka and kill the demon king Ravana. Lord Rama rescues Goddesses Sita and with the help of Lord Hanuman they return back to Ayodhya. A fourteen years had lapsed when lord Rama had left the kingdom of Ayodhya. When lord Rama, Goddesses Sita, Lord Lakshmana and lord Hanuman return to Ayodhya, the people of Ayodhya cheer up and lit up the whole Ayodhya with Diyas in order to celebrate the victory and return of their king Lord Rama. The festival Dusherra is celebrated every year as a victory of good over evil as the demon Meghanad {the son of King Ravana},the demon Kumbhakarna {the brother of king Ravana} and the demon King Ravana of Lanka were killed by Lord Rama and Lord Lakshmana in order to rescue Goddesses Sita from her captivity in Lanka. And on the occassion of their return to Ayodhya the festival Diwali is celebrated throughout India every year with  great pomp and show of lights and crackers. Whenever some troubled minds read this story of Ramayana their minds gain peace and the self confidence is restored as their spirits become calm, pacified and brave in the troubled periods of time. The Epic Ramayana denotes the greatness of Supreme God Lord Rama as an ideal human being on earth.  \"\n",
      "* * * * * * * * * \n",
      "24660\n",
      "this is typical Wikipedia/Who gives a S~~t\n",
      "* * * * * * * * * \n",
      "24991\n",
      "What I would like to know is what the theoretical speed is. Is it a gigabit or a gibibit? And on what layer does it transport that ammount of data? That is, are the ethernet header, CRC and preamble (and anything I forgot) included in the 1 gigabit, or can you send 1 gigabit/s on top of them? So what is the theoretical maximum bytes/s over raw ethernet (so not even using IP) that could be send then?\n",
      "* * * * * * * * * \n",
      "27796\n",
      "So... \n",
      "\n",
      "Are you ever NOT high when you edit?\n",
      "* * * * * * * * * \n",
      "28295\n",
      "Lol, you are too busy doing what now?   Maybe you don't realize it, but no one gives a shit about anything you're doing, because it's a waste of time and no one will ever read it\n",
      "* * * * * * * * * \n",
      "30747\n",
      "Really?\n",
      "Personal attacks/harassment? Or were you just looking for a reason to shut me up and vandalism just wasn't good enough?\n",
      "* * * * * * * * * \n",
      "32153\n",
      "because I have little else to do with my empty life\n",
      "* * * * * * * * * \n",
      "33417\n",
      "Dont invade my life\n",
      "* * * * * * * * * \n",
      "39380\n",
      "Hispanic\n",
      "What the hell do Hispanics have to do with this? I'm erasing it.\n",
      "* * * * * * * * * \n",
      "43188\n",
      "When will somebody do their bloody research and find out about his second child rather than just pretend he doesn't have one?\n",
      "* * * * * * * * * \n",
      "44831\n",
      "I think you'll find they are major duhhhh. Im changing the genre, its offensive and ignorant and we've had several complaints-\n",
      "* * * * * * * * * \n",
      "47624\n",
      "apollo \n",
      "a consensus has being gained on the discussion page 3 vs 1 in favour of roman names. we would like you to do the honours and put them on please. what a day for wikipedias decomracy and anti-bias projects\n",
      "* * * * * * * * * \n",
      "51405\n",
      "These are construtive changes. You are just a troll.\n",
      "* * * * * * * * * \n",
      "51838\n",
      "Hypocrisy \n",
      "\n",
      "If I removed everything that doesn't have a source on WP, there goes half your site! Ironic! But the sad thing is, it's sourced to numerous publications.\n",
      "\n",
      "PS.. Clearly you didn't take my advice with the ice pick. Oh what a shame. \n",
      "\n",
      "207.161.182.225\n",
      "* * * * * * * * * \n",
      "51871\n",
      "It's no wonder this place is hated so much. You feel big now? Grow up.\n",
      "* * * * * * * * * \n",
      "59064\n",
      "My pleasureyou too,\n",
      "* * * * * * * * * \n",
      "59767\n",
      "List of sporting comebacks \n",
      "\n",
      "Hehe. Sorry. )  ?!?\n",
      "* * * * * * * * * \n",
      "60500\n",
      "We Have Our Eyes On You]]\n",
      "* * * * * * * * * \n",
      "60622\n",
      "\"If the \"\"gay community\"\" bans or criticizes a gay organization for political purposes, that does not make the discriminated against organization less gay.  NAMBLA was formed by gays, for the purpose of gay liberation, to fight mistreatment of gays.  All its original founders and celebrity supporters were gay icons.  Until the right wing decided to use NAMBLA to target the gay community, and gays decided that gay rights wouldn't happen with NAMBLA attached to their movement, NAMBLA marched in gay pride parades, and belonged to many gay unbrella organizations.  Gays do not get to cherry pick those parts of gay history which they think will pacify their critics, and retroactively declare some of their former activities \"\"non-gay.\"\"  That would be like the NAACP declaring Jesse Jackson's Rainbow Coalition to not be a part of the Negro community.   \n",
      "\n",
      "\"\n",
      "* * * * * * * * * \n",
      "61549\n",
      "Images\n",
      "\n",
      "Your violation of WP:IUP on Sugababes has been reverted.\n",
      "* * * * * * * * * \n",
      "62510\n",
      "john deere \n",
      "\n",
      "who is john deere's mothers name\n",
      "* * * * * * * * * \n",
      "63808\n",
      "hate is my topic is hate\n",
      "if you hate a person means you like that person. Your hate can turn into your love\n",
      "* * * * * * * * * \n",
      "63929\n",
      "\"\n",
      "\n",
      "Well, if one is hell bent on finding quotes containing \"\"marx\"\", \"\"terrorism\"\", \"\"die\"\", and \"\"kill\"\" together with all possible synonyms in order to place them into a propaganda piece, they ought to be able to find something. Especially if work of revolutionaries could not avoid discussion of violence in human history or nature. And it really does not matter that the quotes are taken out of context or misinterpreted, or metaphors are taken seriously, or some other bull shit. As long as it helps to push propaganda, anything would work, right? (  )\"\n",
      "* * * * * * * * * \n",
      "68900\n",
      "grow up \n",
      "\n",
      "Get a life\n",
      "* * * * * * * * * \n",
      "70271\n",
      "I luv u people!!!!!!!!!!!..................LOL\n",
      "* * * * * * * * * \n",
      "71303\n",
      "Pathetic little tosser. Get a real life instead of wasting it away on here. New IP addresses can be obtained in 20 seconds LOL\n",
      "* * * * * * * * * \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74520\n",
      "We should stick to English usage, as J.Kenney above mentioned, instead of catering to particular nationalities.\n",
      "* * * * * * * * * \n",
      "80578\n",
      "If you don't wish personal ... \n",
      "\n",
      "Where is Sorin  Cerin Mr. J. Mabel Garbage?\n",
      "* * * * * * * * * \n",
      "80612\n",
      "\":Grasp the basics of signing your posts first. '''''' Dick Laurent is dead \n",
      "\n",
      "\"\n",
      "* * * * * * * * * \n",
      "81125\n",
      "Today's high-quality journalism. I am scratching my head as to how to edit this article.\n",
      "* * * * * * * * * \n",
      "81147\n",
      "Blocked\n",
      "\n",
      "You are clearly the same person as  and are continuing with your blatant violations of WP:IUP.  This account has now been blocked indefinitely.\n",
      "* * * * * * * * * \n",
      "83718\n",
      "[User:Bbb23]] you can take your pompous poaturings and berger off.\n",
      "* * * * * * * * * \n",
      "95525\n",
      "I don't hate Jimbo.  I find him very entertaining. -\n",
      "* * * * * * * * * \n",
      "97109\n",
      "\"=Please do not edit user pages==\n",
      "Please do not edit others' user pages. This is considered vandalism. —  দ 23:26, 14 Apr 2005 (UTC)\n",
      "\n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  ==  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  ==  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  ==  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  ==  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "  POLITCAL PIGGY !!!  \n",
      "=\n",
      "* * * * * * * * * \n",
      "99656\n",
      "You offend easily apparently.\n",
      "* * * * * * * * * \n",
      "100354\n",
      "that person put pictures of my dead friends. i will get a court order and have NCIS arrest the CEO of Wikipedia, Sherurcij, and anyone else involved. They were War Heroes and you are putting a picture that is traumitizing for thousands on the internet.\n",
      "* * * * * * * * * \n",
      "107454\n",
      "LAWL \n",
      "\n",
      "lulz @ u nerd, keep deleting my comments.\n",
      "* * * * * * * * * \n",
      "107836\n",
      "Who exactly was Joseph Henry Jackson? Does anyone know?\n",
      "-98.228.122.251\n",
      "* * * * * * * * * \n",
      "110395\n",
      "\"\n",
      "\n",
      "Exist 2 Inspire\n",
      "The Hardys are one of the universis greatest tag teams. They are no longer working together seeing as Jeff Hardy got sacked. He may be making a surprise return to the WWE in Summer or early Fall of 2006. This being because his contract is expiring on TNA (total nonstop action), so the WWE are thinking of bringing back one of the most popular ever wrestlers in the WWE. He may once again form a stable with his brother Matt and bring back the great team extreme. all the hardy boyz fans out there youll be seing them back in action before the end of the year.\n",
      "\n",
      "lita (DDT) jeff (swanton bomb) matt (swist of fate, side effect)\n",
      "              \n",
      "edge and christian are bitches as are the dudly boys\n",
      "\n",
      "facts jeff hardy most gamest person on tna at the moment\n",
      "facts matt hardy most electrifing person on smackdown\n",
      "facts lita most cheating woman on raw at the moment\n",
      "with her fucked up boyfriend edge but after its gonna be \n",
      "lita with jeff and trish status and matt so r u ready\n",
      "for the reunited team extream matt,jeff and lita\n",
      "\n",
      "^^Total garbage, none of it is fact, not even decent speculation^^\n",
      "\n",
      "Lita would not be in Team X-Treme. That would be asking for a problem.\n",
      "\n",
      "Well as of now maybe Jeff will enter a feud with Edge and maybe get kicked off of Raw the way Matt did? Or maybe Edge will get kicked off and continue his feud with Hardy on Smackdown!\n",
      "\n",
      "Exist 2 Inspire\n",
      "Edge and Lita will possibly be leaving to TNA in the future. And the Hardy Boyz Reunion has been talked about happening at Wrestlemania 23.\n",
      "\n",
      "No Rumors.  (talk to me) (watch me) \n",
      "\n",
      "The Hardy reunion has been finalised, they're competing together at Survivor Series, part of the team captained by Dx, along with ECW's Punk. Perhaps that should be entered into the article.   Richard\n",
      "\n",
      " stage name \n",
      "\n",
      "Even, and especially, if you claim that Hardy is their real last name and not a stage name, then there is an unbelievable coincidence then that they chose to be called the Hardy Boyz and not the Hardy Brothers,, the Hardies or whatever. This does not distract from their image, don't take it personally. Please explain why Hardy Boyz has no relation to anything else and was a pure creative idea. \n",
      "\n",
      "I'm sorry if I sound rude, but do you even watch wrestling? Do you have any clue what you are talking about? Hardy is their legal last name. Matt Hardy and Jeff Hardy. So they are called the Hardy Boyz. Jeez, you would think if you wanted to edit a wrestling article, you would at least know what you are talking about. Again, sorry if you take this offensively, but its the truth.  (talk)  \n",
      "\n",
      "While you continue to ignore my argument, the argument you use is childish and preposterous and you really don't care if I watch wresting, which leagues or since when either. Whether Hardy is their real last name or not is not the issue. The facts are that \n",
      "a) whether you like it or not, Hardy Boyz is a stage name, not unlike the Hart Foundation, and the Beverly Brothers, \n",
      "b) the Hardy Boys existed way before the Hardy Boyz adopted this variation. \n",
      "Soon your going to tell me that the name 'Hulk' in Hulk Hogan, has nothing to do with the previous usage regarding the green giant? Nobody has claimed a copyright violation, and I have absolutely no clue, and frankly wikipedia does not care, why you think that you have master editing rights on this article.  \n",
      "First, The Hart Foundation were called that because the real names of the characters are Bret HART and Jim NeidHART. \"\"Hardy\"\" is their legal last name. Please do some research first, since they are sometimes called the Hardy's. Maybe they chose Hardy Boyz instead of Hardy Brothers because of the books, you would have to ask Matt or Jeff that. Their real legal names are Matt Hardy and Jeff Hardy though.  \n",
      "\n",
      "If you would only listen to yourselves, you'd hear that their real name isn't 'Hardy Boyz' at all, and that they've chosen this stage name over \"\"Matt Hardy and Jeff Hardy\"\". TJ, you \"\"maybe they chose...\"\" is the whole point, you and Mike have no other clue. Whatever. Why you both think it detracts from their image is beyond me.  20\n",
      "* * * * * * * * * \n",
      "112269\n",
      "Sources in Portuguese\n",
      "See: http://www.konistore.com.br/konistore/?page_id=24\n",
      "* * * * * * * * * \n",
      "112691\n",
      "with some users I feel are just a little too uptight\n",
      "* * * * * * * * * \n",
      "112818\n",
      "Then grow up and stop spamming on my page.  I hear your mommy calling.  Run along.\n",
      "* * * * * * * * * \n",
      "114125\n",
      "Leftcoastman \n",
      "\n",
      "Proud of your block?  I guess being a liitle unemployed creep, you have little choice but to show off. And again, real evidence based edits are more important than civil POV bullshit. 166.205.138.250\n",
      "* * * * * * * * * \n",
      "114328\n",
      "wikipedia will fall \n",
      "\n",
      "like all things, wikipedia will fall among the rest-not soon enough-It is nothing more than controlled media\n",
      "* * * * * * * * * \n",
      "114669\n",
      "Are you that scared? )\n",
      "* * * * * * * * * \n",
      "115870\n",
      "(UTC)\n",
      "grow up, common usage is allowed.  20:42, 12 July 2005\n",
      "* * * * * * * * * \n",
      "124687\n",
      "News Alert Todst Gets New Barnstar \n",
      "\n",
      "For Being Gay\n",
      "* * * * * * * * * \n",
      "125306\n",
      "lulz \n",
      "\n",
      "i'm 12 and what is this\n",
      "* * * * * * * * * \n",
      "126789\n",
      "British Isles usage\n",
      "* * * * * * * * * \n"
     ]
    }
   ],
   "source": [
    "# false positives\n",
    "\n",
    "print(\"Examples of false positives:\")\n",
    "\n",
    "import random, time\n",
    "\n",
    "for i in range(0, len(svm_predictions)):\n",
    "    if (svm_predictions[i] == 1):\n",
    "        if (X_raw_train.iloc[i]['any_toxic'] == 0):\n",
    "            if (random.uniform(0, 1) < 0.05): # to print only 5% of the false positives\n",
    "                print(i)\n",
    "                print(X_raw_train.iloc[i]['comment_text'])\n",
    "                print('* * * * * * * * * ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set performance\n",
    "\n",
    "Since we are concerned with the minority class (that of positives for toxicity), optimizing precision and the true positive rate is more important that optimizing the true negative rate and accuracy (which is biased towards the majority class - negative for toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 3299, 'Neg': 28616, 'TP': 2155, 'TN': 27972, 'FP': 644, 'FN': 1144, 'Accuracy': 0.9439761867460442, 'Precision': 0.7699178277956413, 'Recall': 0.653228250985147, 'desc': 'svm_test'}\n"
     ]
    }
   ],
   "source": [
    "svm_performance_test = BinaryClassificationPerformance(svm.predict(X_test), y_test, 'svm_test')\n",
    "svm_performance_test.compute_measures()\n",
    "print(svm_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Plot for training and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fits = [svm_performance_train, svm_performance_test]\n",
    "\n",
    "plt.plot(fits[0].performance_measures['FP']/fits[0].performance_measures['Neg'],\n",
    "         fits[0].performance_measures['TP']/fits[0].performance_measures['Pos'], 'bo', color=\"green\")\n",
    "plt.plot(fits[1].performance_measures['FP']/fits[1].performance_measures['Neg'],\n",
    "         fits[1].performance_measures['TP']/fits[1].performance_measures['Pos'], 'bo', color=\"red\")\n",
    "\n",
    "for fit in fits:\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "              fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 0.5, 0, 1])\n",
    "plt.title('ROC plot: training set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBMSSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_data is: <class 'pandas.core.frame.DataFrame'>\n",
      "toxic_data has 153164 rows and 2 columns \n",
      "\n",
      "the data types for each of the columns in toxic_data:\n",
      "id              object\n",
      "comment_text    object\n",
      "dtype: object \n",
      "\n",
      "the first 10 rows in toxic_data:\n",
      "                 id                                       comment_text\n",
      "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
      "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
      "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
      "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
      "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
      "Shape of HashingVectorizer X:\n",
      "(153164, 8192)\n",
      "Look at a few rows of the new quantitative features: \n",
      "   word_count  occur_fuck  occur_shit  punc_count  uppercase_letters\n",
      "0          72           2           1           0                  4\n",
      "1          13           0           0           0                  7\n",
      "2          16           0           0           0                  4\n",
      "3          38           0           0           0                  4\n",
      "4           7           0           0           0                  1\n",
      "5          16           0           0           0                  2\n",
      "6          31           0           0           0                  5\n",
      "7           6           0           0           0                  1\n",
      "8         109           0           0           0                 41\n",
      "9          41           0           0           0                  7\n",
      "Size of combined bag of words and new quantitative variables matrix:\n",
      "(153164, 8197)\n",
      "(153164, 8197)\n",
      "Shape of X_test for submission:\n",
      "(153164, 8197)\n",
      "SUCCESS!\n",
      "Number of rows in the submission test set (should be 153,164): \n"
     ]
    }
   ],
   "source": [
    "raw_data, X_test_submission = process_raw_data(fn='toxiccomments_test.csv', my_random_seed=80, test=True)\n",
    "print(\"Number of rows in the submission test set (should be 153,164): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19500013057898724\n"
     ]
    }
   ],
   "source": [
    "# store the id from the raw data\n",
    "my_submission = pd.DataFrame(raw_data[\"id\"])\n",
    "# concatenate predictions to the id\n",
    "my_submission[\"prediction\"] = svm.predict(X_test_submission)\n",
    "# look at the proportion of positive predictions\n",
    "print(my_submission['prediction'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>occur_fuck</th>\n",
       "      <th>occur_shit</th>\n",
       "      <th>uppercase_letters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...   \n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all.   \n",
       "5  0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "6  00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...   \n",
       "7  000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "8  00025358d4737918  \" \\n Only a fool can believe in such numbers. ...   \n",
       "9  00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...   \n",
       "\n",
       "   word_count  punc_count  occur_fuck  occur_shit  uppercase_letters  \n",
       "0          72           0           2           1                  4  \n",
       "1          13           0           0           0                  7  \n",
       "2          16           0           0           0                  4  \n",
       "3          38           0           0           0                  4  \n",
       "4           7           0           0           0                  1  \n",
       "5          16           0           0           0                  2  \n",
       "6          31           0           0           0                  5  \n",
       "7           6           0           0           0                  1  \n",
       "8         109           0           0           0                 41  \n",
       "9          41           0           0           0                  7  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  prediction\n",
       "0  00001cee341fdb12        True\n",
       "1  0000247867823ef7       False\n",
       "2  00013b17ad220c46       False\n",
       "3  00017563c3f7919a       False\n",
       "4  00017695ad8997eb       False\n",
       "5  0001ea8717f6de06       False\n",
       "6  00024115d4cbde0f       False\n",
       "7  000247e83dcc1211        True\n",
       "8  00025358d4737918       False\n",
       "9  00026d1092fe71cc       False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export submission file as pdf\n",
    "# CHANGE FILE PATH: \n",
    "my_submission.to_csv('sample_submissions/toxiccomments_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
